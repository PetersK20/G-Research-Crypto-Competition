# -*- coding: utf-8 -*-
"""lstm-single-model*.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19vwb2TYSMLsrfk21T9C8CVWi-PonlaKJ
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import gc
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.autograd import Variable 
from tqdm import tqdm
import math
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import seaborn as sbn
import math
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
from datetime import datetime
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
!pip install talib-binary
import talib
# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

def reduce_mem_usage(df):
    """ iterate through all the columns of a dataframe and modify the data type
        to reduce memory usage.        
    """
    start_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))
    
    for col in df.columns:
        try:
            col_type = df[col].dtype

            if col_type != object:
                c_min = df[col].min()
                c_max = df[col].max()
                if str(col_type)[:3] == 'int':
                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                        df[col] = df[col].astype(np.int8)
                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                        df[col] = df[col].astype(np.int16)
                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                        df[col] = df[col].astype(np.int32)
                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                        df[col] = df[col].astype(np.int64)  
                else:
                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                        df[col] = df[col].astype(np.float16)
                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                        df[col] = df[col].astype(np.float32)
                    else:
                        df[col] = df[col].astype(np.float64)
        except:
            continue

    end_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))
    
    return df

from google.colab import drive
drive.mount('/content/drive')
! pip install kaggle
! mkdir ~/.kaggle
! cp /content/drive/MyDrive/kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

! kaggle competitions download g-research-crypto-forecasting --force

combined = pd.read_csv("/content/train.csv.zip")

asset_details = pd.read_csv("/content/asset_details.csv")
#asset id maps to the training set with the time series data.  The weight of the cryptocurrency is also listed
asset_details.index = asset_details["Asset_ID"]
combined['datetime'] = pd.to_datetime(combined['timestamp'], unit='s')
combined["Asset_Name"] = asset_details.loc[combined.loc[:,"Asset_ID"],"Asset_Name"].to_numpy()
#combined.loc[combined["Target"].isna(),"Target"]=combined["Target"].mean(skipna=True)

ids = list(asset_details.Asset_ID)
asset_names = list(asset_details.Asset_Name)
all_timestamps = np.sort(combined['timestamp'].unique())
targets = pd.DataFrame(index=all_timestamps)

ids = list(asset_details.Asset_ID)
asset_names = list(asset_details.Asset_Name)
all_timestamps = np.sort(combined['timestamp'].unique())
targets = pd.DataFrame(index=all_timestamps)

def calculate_target(data: pd.DataFrame, details: pd.DataFrame, price_column: str):
    ids = list(details.Asset_ID)
    asset_names = list(details.Asset_Name)
    weights = np.array(list(details.Weight))

    all_timestamps = np.sort(data['timestamp'].unique())
    targets = pd.DataFrame(index=all_timestamps)

    for i, id in enumerate(ids):
        asset = data[data.Asset_ID == id].set_index(keys='timestamp')
        price = pd.Series(index=all_timestamps, data=asset[price_column])
        targets[asset_names[i]] = (
            price.shift(periods=0) /
            price.shift(periods=15)
        ) - 1
    
    targets['marketReturn'] = np.average(targets.fillna(0), axis=1, weights=weights)
    
    m = targets['marketReturn']

    num = targets.multiply(m.values, axis=0).rolling(3734).mean().values
    denom = m.multiply(m.values, axis=0).rolling(3734).mean().values
    beta = np.nan_to_num(num.T / denom, nan=0., posinf=0., neginf=0.)

    for i, id in enumerate(ids):
      targets["BetaRolling"+asset_names[i]] = beta[i,:]




    targets['datetime'] = pd.to_datetime(targets.index, unit='s')
    targets["year"] = pd.DatetimeIndex(targets['datetime']).year
    targets["month"] = pd.DatetimeIndex(targets['datetime']).month


    return targets

targets = calculate_target(combined,asset_details,"Close")

def createDataset2(data,suffix):

    data["Target"+suffix] = data["Target"]
    data["Target"+suffix+"2"] = np.log(data["Close"].shift(periods=-16)) - np.log(data["Close"].shift(periods=-1))
    data["Open"+suffix] = np.log(data["Open"]) - np.log(data["Open"].shift(periods=1))
    data["Low"+suffix] = np.log(data["Low"]) - np.log(data["Low"].shift(periods=1))
    data["High"+suffix] = np.log(data["High"]) - np.log(data["High"].shift(periods=1))
    data["VWAP"+suffix] = np.log(data["VWAP"]) - np.log(data["VWAP"].shift(periods=1))
    data["CloseLog"+suffix] = np.log(data["Close"])
    
    data["Close"+suffix] = np.log(data["Close"]) - np.log(data["Close"].shift(periods=1))
    
    data["Volume"+suffix] = np.log(data["Volume"]) - np.log(data["Volume"].shift(periods=1))
    data["Count"+suffix] = np.log(data["Count"]) - np.log(data["Count"].shift(periods=1))

    return data[["Target"+suffix,"Open"+suffix,"Low"+suffix,"High"+suffix,"VWAP"+suffix,"Close"+suffix,"CloseLog"+suffix,"Target"+suffix+"2","Volume"+suffix,"Count"+suffix,"datetime"]]
def createDataset3(targets):
    data = combined.loc[combined["Asset_Name"]=="Bitcoin",:].reset_index(drop=True) 
    data = createDataset2(data, "Bitcoin")
    targets=pd.merge(targets,data, how="left",on="datetime")


    data = combined.loc[combined["Asset_Name"]=="Bitcoin Cash",:].reset_index(drop=True)
    data = createDataset2(data, "Bitcoin Cash")
    targets=pd.merge(targets,data,how="left",on="datetime")

    data = combined.loc[combined["Asset_Name"]=="Ethereum",:].reset_index(drop=True)
    data = createDataset2(data, "Ethereum")
    targets=pd.merge(targets,data,how="left",on="datetime")

    data = combined.loc[combined["Asset_Name"]=="Binance Coin",:].reset_index(drop=True)
    data = createDataset2(data, "Binance Coin")
    targets=pd.merge(targets,data,how="left",on="datetime")

    data = combined.loc[combined["Asset_Name"]=="EOS.IO",:].reset_index(drop=True)
    data = createDataset2(data, "EOS.IO")
    targets=pd.merge(targets,data,how="left",on="datetime")

    data = combined.loc[combined["Asset_Name"]=="Ethereum Classic",:].reset_index(drop=True)
    data = createDataset2(data, "Ethereum Classic")
    targets=pd.merge(targets,data,how="left",on="datetime")

    data = combined.loc[combined["Asset_Name"]=="Litecoin",:].reset_index(drop=True)
    data = createDataset2(data, "Litecoin")
    targets=pd.merge(targets,data,how="left",on="datetime")
    
    
    
    
    data = combined.loc[combined["Asset_Name"]=="Monero",:].reset_index(drop=True)
    data = createDataset2(data, "Monero")
    targets=pd.merge(targets,data,how="left",on="datetime")
    
    data = combined.loc[combined["Asset_Name"]=="TRON",:].reset_index(drop=True)
    data = createDataset2(data, "TRON")
    targets=pd.merge(targets,data,how="left",on="datetime")
    
    data = combined.loc[combined["Asset_Name"]=="Stellar",:].reset_index(drop=True)
    data = createDataset2(data, "Stellar")
    targets=pd.merge(targets,data,how="left",on="datetime")
    
    data = combined.loc[combined["Asset_Name"]=="Cardano",:].reset_index(drop=True)
    data = createDataset2(data, "Cardano")
    targets=pd.merge(targets,data,how="left",on="datetime")
    
    data = combined.loc[combined["Asset_Name"]=="IOTA",:].reset_index(drop=True)
    data = createDataset2(data, "IOTA")
    targets=pd.merge(targets,data,how="left",on="datetime")
    
    data = combined.loc[combined["Asset_Name"]=="Maker",:].reset_index(drop=True)
    data = createDataset2(data, "Maker")
    targets=pd.merge(targets,data,how="left",on="datetime")
    
    data = combined.loc[combined["Asset_Name"]=="Dogecoin",:].reset_index(drop=True)
    data = createDataset2(data, "Dogecoin")
    targets=pd.merge(targets,data,how="left",on="datetime")


    return targets

targets = createDataset3(targets)

timeSeriesFeatures = []
hiddenFeatures = []
targetVar=[]
targetVar2=[]
for suffix in ["Bitcoin","Bitcoin Cash","Ethereum","Binance Coin","EOS.IO","Ethereum Classic","Litecoin","Monero","TRON","Stellar","Cardano","IOTA","Maker","Dogecoin"]:
    targetVar.append("Target"+suffix)
    targetVar2.append("Target"+suffix+"2")
    for i in ["Open","High","Low","VWAP","Close","Volume","Count","BetaRolling"]:
        timeSeriesFeatures.append(i+suffix)
    hiddenFeatures.append("CloseLog"+suffix)
del combined

targets=targets.replace([np.inf,-np.inf],np.nan)
targets[timeSeriesFeatures+hiddenFeatures] = targets[timeSeriesFeatures+hiddenFeatures].fillna(method="ffill")
targets[timeSeriesFeatures+hiddenFeatures] = targets[timeSeriesFeatures+hiddenFeatures].fillna(method="bfill")
targets = targets[3750:-16]

#from sklearn.preprocessing import StandardScaler

#targets=targets[targets["year"].isin([2018,2019,2020,2021])]
#std_scaler = StandardScaler()
#targets.loc[:,timeSeriesFeatures[:30]] = std_scaler.fit_transform(targets[timeSeriesFeatures[:30]])
#targets.loc[:,timeSeriesFeatures[30:]] = std_scaler.fit_transform(targets[timeSeriesFeatures[30:]])
#bitCoinData.loc[:,HiddenFeatures] = std_scaler.fit_transform(bitCoinData[HiddenFeatures])
#targets.loc[:,targetVar] = std_scaler.fit_transform(targets.loc[:,targetVar])
#targetMean = np.nanmean(targets.loc[:,targetVar2])
#targetStd = np.nanstd(targets.loc[:,targetVar2])
#targets.loc[:,targetVar2] = (targets.loc[:,targetVar2]- targetMean))/targetStd

weights = np.array([asset_details.loc[1,"Weight"], asset_details.loc[2,"Weight"], 
                    asset_details.loc[6,"Weight"], asset_details.loc[0,"Weight"], 
                    asset_details.loc[5,"Weight"], asset_details.loc[7,"Weight"],
                    asset_details.loc[9,"Weight"],asset_details.loc[11,"Weight"],
                    asset_details.loc[13,"Weight"],asset_details.loc[12,"Weight"],
                    asset_details.loc[3,"Weight"],asset_details.loc[8,"Weight"],
                    asset_details.loc[10,"Weight"],asset_details.loc[4,"Weight"]
                   ])

#hiddenTensor = Variable(torch.tensor(np.array(bitCoinData.loc[:,HiddenFeatures])).type('torch.FloatTensor'))#.cuda()
targetTensor = Variable(torch.tensor(np.array(targets.loc[:,targetVar])).type('torch.FloatTensor'))#.cuda()
targetTensor = torch.nan_to_num(targetTensor, -100)

targetTensor2 = Variable(torch.tensor(np.array(targets.loc[:,targetVar2])).type('torch.FloatTensor'))#.cuda()
targetTensor2 = torch.nan_to_num(targetTensor2, -100)

year = targets.year
month = targets.month


dataTensor = Variable(torch.tensor(np.array(targets[timeSeriesFeatures+hiddenFeatures])).type('torch.FloatTensor'))

gc.collect()

class EarlyStopping:
  def __init__(self, path, patience, minEpoch, maxEpoch):
    self.path=path
    self.trainPreds=[]
    self.testPreds=[]
    self.valPreds=[]
    self.bestCor = 0
    self.bestEpoch = 0
    self.epoch = -1
    self.numEpochsNotImpove = 0
    self.patience = patience
    self.minEpoch = minEpoch
    self.maxEpoch = maxEpoch
  def addPreds(self,trainPreds,testPreds,valPreds):
    self.trainPreds.append(trainPreds)
    self.testPreds.append(testPreds)
    self.valPreds.append(valPreds)
  def addEpoch(self, lstm, valAvgCor):
    self.epoch+=1
    if self.bestCor<=valAvgCor or self.epoch<self.minEpoch and self.epoch != self.maxEpoch-1:
      self.bestCor = valAvgCor
      self.numEpochsNotImpove=0
      self.bestEpoch=self.epoch
    elif self.numEpochsNotImpove==self.patience or self.epoch == self.maxEpoch-1:
      print("Early Stopping "+str(self.bestEpoch)+" "+self.bestCor)
      torch.save(lstm, self.path)
      return 1
    else:
      self.numEpochsNotImpove+=1

lag1 = 40
lag1Step = 1
lag2 = 60
lag2Step = 2
lag5 = 110
lag5Step = 5
lag10 = 210
lag10Step = 10
lag60 = 810-60
lag60Step = 60

def returnList(index):
    return list(range((index-lag1+lag1Step),index+lag1Step))
lag = len(returnList(1000))
maxLag = lag1-1

def scaleTensor(train, val, test):
    stdBeta = np.array(train[:,list(range(7,14*8,8))]).std()
    for i in range(len(timeSeriesFeatures)):
        if i % 8 == 7:
            train[:,i] = (train[:,i])/stdBeta
            val[:,i] = (val[:,i])/stdBeta
            test[:,i] = (test[:,i])/stdBeta
        else:
            mean = train[:,i].mean()
            std = train[:,i].std()
            train[:,i] = (train[:,i]-mean)/std
            val[:,i] = (val[:,i]-mean)/std
            test[:,i] = (test[:,i]-mean)/std

    return train, val, test

def scaleTargets(train, val, test):
    nanIndexes = train != -100
    mean = train[nanIndexes].mean()
    std = train[nanIndexes].std()
    
    train = (train)/std
    train[~nanIndexes]=-100

    nanIndexes = val != -100
    val = (val)/std
    val[~nanIndexes]=-100

    nanIndexes = test != -100
    test = (test)/std
    test[~nanIndexes]=-100

    return train, val, test

def getDataSplit(startTestDate, months, months2, valBefore):
    indexes = np.array(range(dataTensor.shape[0]))

    datetime = targets.datetime
    startIndex = indexes[(datetime==startTestDate)][0] #"2021-06-13 00:00:00"

    trainSize = months*30*24*60
    testSize = months2*30*24*60
    testEnd = 0
    trainEnd = 0
    trainStart = 0
    testStart =0
    if valBefore:
      testStart = startIndex- (testSize+trainSize)
      trainStart = startIndex-trainSize
      testEnd=startIndex-trainSize
      trainEnd=startIndex
    else:
      trainStart = startIndex-(testSize+trainSize)
      testStart = startIndex-testSize
      testEnd = startIndex
      trainEnd=startIndex-testSize


    trainBitcoin = dataTensor[np.array(range(trainStart,trainEnd,1))]
    trainTarget = targetTensor[np.array(range(trainStart,trainEnd,1))]
    trainTarget2 = targetTensor2[np.array(range(trainStart,trainEnd,1))]


    valBitcoin = dataTensor[np.array(range(testStart,testEnd,1))]
    valTarget = targetTensor[np.array(range(testStart,testEnd,1))]
    valTarget2 = targetTensor2[np.array(range(testStart,testEnd,1))]



    endIndex = startIndex+129600
    endIndex = endIndex if endIndex< len(indexes) else len(indexes)
    testBitcoin = dataTensor[np.array(range(startIndex,endIndex,1))]
    testTarget = targetTensor[np.array(range(startIndex,endIndex,1))]
    testTarget2 = targetTensor2[np.array(range(startIndex,endIndex,1))]


    trainBitcoin, valBitcoin, testBitcoin = scaleTensor(trainBitcoin, valBitcoin, testBitcoin)
    trainTarget, valTarget, testTarget = scaleTargets(trainTarget, valTarget, testTarget)
    trainTarget2, valTarget2, testTarget2 = scaleTargets(trainTarget2, valTarget2, testTarget2)


    params = {'batch_size': 128,
              'shuffle': True,
            }
    trainIndexes = list(range(maxLag,trainBitcoin.shape[0]))
    sequenceIndex = [returnList(index) for index in trainIndexes] 
    dataset_training = TensorDataset(torch.tensor(sequenceIndex))
    training_loader = DataLoader(dataset_training, **params)

    params = {'batch_size': 500,
              'shuffle': False,
            }

    valIndexes = list(range(maxLag,valBitcoin.shape[0]))
    sequenceIndex = [returnList(index) for index in valIndexes] 
    dataset_val = TensorDataset(torch.tensor(sequenceIndex))
    val_loader = DataLoader(dataset_val, **params)

    testIndexes = list(range(maxLag,testBitcoin.shape[0]))
    sequenceIndex = [returnList(index) for index in testIndexes] 
    dataset_test = TensorDataset(torch.tensor(sequenceIndex))
    testing_loader = DataLoader(dataset_test, **params)

    return training_loader, val_loader, testing_loader, trainIndexes, valIndexes, testIndexes,trainBitcoin, valBitcoin, testBitcoin,trainTarget,trainTarget2, valTarget, valTarget2, testTarget, testTarget2

class LSTMCustom(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(LSTMCustom, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.num_classes = num_classes
        self.ReLU = nn.ReLU()


        #Try combing with output of lstm
        self.lstmList = nn.ModuleList([])
        self.fcList = nn.ModuleList([])
        self.fcOutList = nn.ModuleList([])
        self.secList = nn.ModuleList([])
        for i in range(14):
            self.lstmList.append(
                nn.LSTM(input_size = 7, hidden_size = hidden_size, num_layers = num_layers, batch_first = True)
            )
            self.fcList.append(
                nn.Sequential(nn.Linear(hidden_size, 256),nn.Dropout(.45),nn.ReLU(),nn.Linear(256, 128),nn.Dropout(.15),nn.ReLU())
            )
            self.fcOutList.append(
                nn.Sequential(nn.Linear(128*2+1, 256),nn.Dropout(.35),nn.ReLU(),nn.Linear(256, 1))
            )
            self.secList.append(
                nn.Sequential(nn.Linear(128, 128),nn.Dropout(.35),nn.ReLU(),nn.Linear(128, 1))
            )
        self.marketEncoder = nn.Sequential(nn.Linear(128*14+14, 256),nn.Dropout(.45),nn.ReLU(),nn.Linear(256, 128),nn.Dropout(.15),nn.ReLU())
        self.marketDecoder = nn.Sequential(nn.Linear(128, 128),nn.Dropout(.45),nn.ReLU(),nn.Linear(128, 1))


    def forward(self, data, targetNa, isTrain=False):


        data = data.cuda() 
        if isTrain:
            data = data + torch.cuda.FloatTensor(data.shape).normal_()*.4
    
    
        numFeaturesPerCoin = 8

        outFeatArray = []
        outSecondaryArray = []
        for i in range(14):
            data1 = data[:,:,numFeaturesPerCoin*i:numFeaturesPerCoin*(i+1)-1]
            out, (hn, cnn) = self.lstmList[i](data1,(Variable(torch.zeros(self.num_layers, data.size(0), self.hidden_size)).to(device),Variable(torch.zeros(self.num_layers, data.size(0), self.hidden_size)).to(device)))
            out = self.fcList[i](hn[self.num_layers-1])
            
            outFeatArray.append(out)

            out = self.secList[i](out)
            outSecondaryArray.append(out)

      
        

        outHolder = []

        hiddenVals=torch.cat(outFeatArray,1) 
        hiddenVals = torch.cat((hiddenVals,targetNa),1)
        outMarketEncoder= self.marketEncoder(hiddenVals)
        outMarketDencoder= self.marketDecoder(outMarketEncoder)


        
        outHolder = []
        for i in range(14):
            x = torch.cat((outMarketEncoder,outFeatArray[i],data[:,lag-1,i*8+7:i*8+8]),1)
            x=self.fcOutList[i](x)
            outHolder.append(x)

        
        out = torch.cat(outHolder,1)
        
        out2 = torch.cat(outSecondaryArray,1)
        
        return out, out2, outMarketDencoder

def weighted_correlation(a, b, weights):
    

    a = np.ravel(a)
    b = np.ravel(b)
    w = np.ravel(weights)
   
    sum_w = np.sum(w)
    mean_a = np.sum(a * w) / sum_w
    mean_b = np.sum(b * w) / sum_w
    var_a = np.sum(w * np.square(a - mean_a)) / sum_w
    var_b = np.sum(w * np.square(b - mean_b)) / sum_w

    cov = np.sum((a * b * w)) / np.sum(w) - mean_a * mean_b
    corr = cov / np.sqrt(var_a * var_b)

    return corr
def getLoss(predictions,labels, predictions2,labels2, marketProbs, epoch, tensor):
    indexList = [(labels[:,i]!=-100) for i in range(14)]
    indexList2 = [labels2[:,i]!=-100 for i in range(14)]  

    labels2[labels2==-100]=0
    marketLabel = (labels2*weights).sum(axis=1)/weights.sum() #* tensor[:,list(range(7,8*14,8))]


    loss=0
    weight = torch.ones(labels.shape)
    weight = weight*weights
    loss+=wcorr_loss(predictions[labels!=-100].reshape(-1), labels[labels!=-100].reshape(-1).cuda(),weight[labels!=-100].reshape(-1).cuda())

    wSum = np.sum(weights[0:14]) /4
    for i in range(14):
        if indexList[i].sum().item()!=0:
            loss+=criterion(predictions[indexList[i],i].reshape(-1), labels[indexList[i],i].reshape(-1).cuda()) *np.mean(weights)

        if indexList2[i].sum().item()!=0:
            loss+=criterion(predictions2[indexList2[i],i].reshape(-1), labels2[indexList2[i],i].reshape(-1).cuda())*wSum
    
    loss+= wSum * criterion(marketProbs.reshape(-1), marketLabel.reshape(-1).cuda())

    return loss

def findCorr(totalProbs,labelsTotal, totalProbs2, labelsTotal2, marketProbs, tensor):

    labelCopy = labelsTotal2.copy()
    labelCopy[labelCopy==-100]=0
    marketLabel = (labelCopy*weights).sum(axis=1)/weights.sum()
    marketCorr = np.corrcoef(marketProbs, marketLabel)[0,1]

    corr = np.array([np.corrcoef(totalProbs[labelsTotal[:,0]!=-100,0], labelsTotal[labelsTotal[:,0]!=-100,0])[0,1], 
                     np.corrcoef(totalProbs[labelsTotal[:,1]!=-100,1], labelsTotal[labelsTotal[:,1]!=-100,1])[0,1], 
                     np.corrcoef(totalProbs[labelsTotal[:,2]!=-100,2], labelsTotal[labelsTotal[:,2]!=-100,2])[0,1], 
                     np.corrcoef(totalProbs[labelsTotal[:,3]!=-100,3], labelsTotal[labelsTotal[:,3]!=-100,3])[0,1], 
                     np.corrcoef(totalProbs[labelsTotal[:,4]!=-100,4], labelsTotal[labelsTotal[:,4]!=-100,4])[0,1], 
                     np.corrcoef(totalProbs[labelsTotal[:,5]!=-100,5], labelsTotal[labelsTotal[:,5]!=-100,5])[0,1], 
                     np.corrcoef(totalProbs[labelsTotal[:,6]!=-100,6], labelsTotal[labelsTotal[:,6]!=-100,6])[0,1],
                     
                     np.corrcoef(totalProbs[labelsTotal[:,7]!=-100,7], labelsTotal[labelsTotal[:,7]!=-100,7])[0,1], 
                     np.corrcoef(totalProbs[labelsTotal[:,8]!=-100,8], labelsTotal[labelsTotal[:,8]!=-100,8])[0,1], 
                     np.corrcoef(totalProbs[labelsTotal[:,9]!=-100,9], labelsTotal[labelsTotal[:,9]!=-100,9])[0,1], 
                     np.corrcoef(totalProbs[labelsTotal[:,10]!=-100,10], labelsTotal[labelsTotal[:,10]!=-100,10])[0,1], 
                     np.corrcoef(totalProbs[labelsTotal[:,11]!=-100,11], labelsTotal[labelsTotal[:,11]!=-100,11])[0,1], 
                     np.corrcoef(totalProbs[labelsTotal[:,12]!=-100,12], labelsTotal[labelsTotal[:,12]!=-100,12])[0,1], 
                     np.corrcoef(totalProbs[labelsTotal[:,13]!=-100,13], labelsTotal[labelsTotal[:,13]!=-100,13])[0,1]])
    weightAverage =(corr*weights).sum() / weights.sum()
    weight = np.ones(totalProbs.shape)
    weight = weight*weights
    return weighted_correlation(totalProbs[labelsTotal!=-100],labelsTotal[labelsTotal!=-100],weight[labelsTotal!=-100]),weightAverage, marketCorr, tuple(corr)
def wcorr_loss(y_true, y_pred, w):
    x = y_true
    y = y_pred
    w = w
    wmx = torch.sum(torch.multiply(x, w)) / torch.sum(w)
    wmy = torch.sum(torch.multiply(y, w)) / torch.sum(w)
    xm, ym = x-wmx, y-wmy
    tfwcovxy = torch.sum( torch.multiply(torch.multiply(xm, w),  ym)) / torch.sum(w)
    tfwcovxx = torch.sum( torch.multiply(torch.multiply(xm, w),  xm)) / torch.sum(w)
    tfwcovyy = torch.sum( torch.multiply(torch.multiply(ym, w),  ym)) / torch.sum(w)
    r = tfwcovxy / torch.sqrt(torch.multiply(tfwcovxx , tfwcovyy))
    r = torch.maximum(torch.minimum(r, torch.cuda.FloatTensor([1.0])), torch.cuda.FloatTensor([-1.0]))
    return - r

def wcorr_fn(w):
    def wcorr_eval(y_true, y_pred):
        return wcorr_loss(y_true, y_pred, w)

training_loader, val_loader, testing_loader, trainIndexes, valIndexes, testIndexes,trainBitcoin, valBitcoin, testBitcoin,trainTarget,trainTarget2, valTarget, valTarget2,testTarget,testTarget2  = getDataSplit("2021-03-15 00:00:00", 6, 3, True )  #"2021-03-15 00:00:00" "2020-12-15 00:00:00"
#stdHolder = findStd(featureIndex)

#remove wcorr loss and 12 months
PATH = "my_model_best_loss.pth"
valTargetNumpy = valTarget.numpy()
testTargetNumpy = testTarget.numpy()
valTargetNumpy2 = valTarget2.numpy()
testTargetNumpy2 = testTarget2.numpy()


numCoins=14
max_epochs = 100
lr = .00001
numFeatures = len(timeSeriesFeatures)//numCoins
hidden = 160
numLayers = 1
num_classes=2

earlyStopping = EarlyStopping(PATH,5,5,25)

device="cuda:0"
lstm = LSTMCustom(numFeatures, hidden, numLayers, num_classes).to(device)
best_loss=100000
epochesNotImprove=0
out_params = list(map(lambda x: x[1],list(filter(lambda kv: "fcOutList" in kv[0], lstm.named_parameters()))))
lstm_params = list(map(lambda x: x[1],list(filter(lambda kv: "fcOutList" not in kv[0], lstm.named_parameters()))))
optimizer = optim.Adam([{'params':lstm_params, 'lr': .00004 }, 
                         {'params':out_params, 'lr': .00004}])
tensorShape = (-1,14)
criterion=nn.L1Loss()
totalProbs=[]
lstm.train()
for epoch in range(max_epochs):
    totalLoss = 0
    totalProbs = []
    totalProbs2 = []
    marketProbs = []
    labelsTotal = []
    labelsTotal2 = []
    lstm.train()
    
    for step, batch in enumerate(tqdm(training_loader)):
        
        batch=batch[0]
        indexes = batch[:,lag-1] 
        data = []
        for i in range(batch.shape[0]):
            data.append(trainBitcoin[batch[i]])
        data = torch.stack(data)
        labels = trainTarget[indexes]
        labels2 = trainTarget2[indexes]
        optimizer.zero_grad()
        probs, probs2, market = lstm(data,(labels==-100).long().cuda(),isTrain=True)
        #del data
        totalProbs.append(probs.cpu().detach().numpy())
        totalProbs2.append(probs2.cpu().detach().numpy())
        marketProbs.append(market.cpu().detach().numpy())
        labelsTotal.append(labels.detach().numpy())
        labelsTotal2.append(labels2.detach().numpy())
   

        loss = getLoss(probs,labels, probs2, labels2, market, epoch,trainBitcoin[indexes,:])
        totalLoss += float(loss)
        loss.backward()
        optimizer.step()
 
    totalProbs=np.concatenate(totalProbs,axis=0).reshape(tensorShape)
    totalProbs2 = np.concatenate(totalProbs2,axis=0).reshape(tensorShape)
    marketProbs = np.concatenate(marketProbs,axis=0).reshape(-1)

    labelsTotal=np.concatenate(labelsTotal,axis=0).reshape((-1,14))
    labelsTotal2=np.concatenate(labelsTotal2,axis=0).reshape((-1,14))
    trainCorrAvg, trainCorrWeighted, trainWeightAverage, (trainCorr, trainCorr2, trainCorr3, trainCorr4, trainCorr5, trainCorr6, trainCorr7,trainCorr8, trainCorr9, trainCorr10, trainCorr11, trainCorr12, trainCorr13, trainCorr14) = findCorr(totalProbs,labelsTotal,totalProbs2,labelsTotal2,marketProbs,trainBitcoin[trainIndexes,:])

    torch.cuda.empty_cache()

    valProbs = []
    valProbs2 = []
    marketProbs = []
    valLoss = 0
    lstm.eval()
    with torch.no_grad():
        for step, batch in enumerate(val_loader):
        
            batch=batch[0]
            batch=np.array(batch)
            indexes = batch[:,lag-1] 

            data = []
            for i in range(batch.shape[0]):
                data.append(valBitcoin[batch[i]])

            data = torch.stack(data)
            labels = valTarget[indexes]
            labels2 = valTarget2[indexes]
                
            predictions, probs2, market = lstm(data,(labels==-100).long().cuda())
            del data
            valProbs.append(predictions.cpu().detach().numpy())
            valProbs2.append(probs2.cpu().detach().numpy())
            marketProbs.append(market.cpu().detach().numpy())
            
            loss = getLoss(predictions,labels, probs2, labels2, market, epoch,valBitcoin[indexes,:])
            valLoss += float(loss)
    

    valProbs = np.concatenate(valProbs,axis=0).reshape(tensorShape)
    valProbs2 = np.concatenate(valProbs2,axis=0).reshape(tensorShape)
    marketProbs = np.concatenate(marketProbs,axis=0).reshape(-1)

    valCorrAvg, valCorrWeighted, valWeightAverage,(valCorr, valCorr2, valCorr3, valCorr4, valCorr5, valCorr6, valCorr7,valCorr8, valCorr9, valCorr10, valCorr11, valCorr12, valCorr13, valCorr14) = findCorr(valProbs,valTargetNumpy[valIndexes],valProbs2,valTargetNumpy2[valIndexes],marketProbs,valBitcoin[valIndexes,:]) 

    
    testProbs = []
    testProbs2 = []
    marketProbs = []
    testLoss = 0
    lstm.eval()
    with torch.no_grad():
        for step, batch in enumerate(testing_loader):
            batch=batch[0]
            batch=np.array(batch)
            indexes = batch[:,lag-1] 

            data = []
            for i in range(batch.shape[0]):
                data.append(testBitcoin[batch[i]])

            data = torch.stack(data)
            labels = testTarget[indexes]
            labels2 = testTarget2[indexes]
            predictions, probs2, market = lstm(data,(labels==-100).long().cuda())
            del data
            testProbs.append(predictions.cpu().detach().numpy())
            testProbs2.append(probs2.cpu().detach().numpy())
            marketProbs.append(market.cpu().detach().numpy())
            
            loss = getLoss(predictions,labels, probs2, labels2, market, epoch,testBitcoin[indexes,:])
            testLoss += float(loss)
    

    testProbs=np.concatenate(testProbs,axis=0).reshape(tensorShape)
    testProbs2 = np.concatenate(testProbs2,axis=0).reshape(tensorShape)
    marketProbs = np.concatenate(marketProbs,axis=0).reshape(-1)

    testCorrAvg, testCorrWeighted, testWeightAverage, (testCorr, testCorr2, testCorr3, testCorr4, testCorr5, testCorr6, testCorr7,testCorr8, testCorr9, testCorr10, testCorr11, testCorr12, testCorr13, testCorr14) = findCorr(testProbs,testTargetNumpy[testIndexes],testProbs2,testTargetNumpy2[testIndexes],marketProbs,testBitcoin[testIndexes,:]) 

    earlyStopping.addPreds(totalProbs,testProbs,valProbs)
        
    print("Epoch: %d, Train accuracy: %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f" % (epoch, trainCorrAvg, trainCorrWeighted, trainWeightAverage, trainCorr, trainCorr2, trainCorr3, trainCorr4, trainCorr5, trainCorr6, trainCorr7,trainCorr8, trainCorr9, trainCorr10, trainCorr11, trainCorr12, trainCorr13, trainCorr14))
    print("Epoch: %d, val accuracy:   %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f" % (epoch, valCorrAvg, valCorrWeighted, valWeightAverage, valCorr, valCorr2, valCorr3, valCorr4, valCorr5, valCorr6, valCorr7,valCorr8, valCorr9, valCorr10, valCorr11, valCorr12, valCorr13, valCorr14))
    print("Epoch: %d, Test accuracy:  %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f %1.5f" % (epoch, testCorrAvg, testCorrWeighted, testWeightAverage, testCorr, testCorr2, testCorr3, testCorr4, testCorr5, testCorr6, testCorr7,testCorr8, testCorr9, testCorr10, testCorr11, testCorr12, testCorr13, testCorr14))
    print("Epoch: %d, Train loss:     %1.5f, Val loss: %1.5f, Test loss: %1.5f" % (epoch, totalLoss/len(training_loader), valLoss/len(val_loader), testLoss/len(testing_loader)))   
    #if(earlyStopping.addEpoch(lstm,valCorrWeighted)):
    #  break
    for g in optimizer.param_groups:
      g["lr"]=g["lr"]*.85
    """
    i=1
    s = "LSTM LR: "
    for g in optimizer.param_groups:
        if i==1:
            if g["lr"]<.00002/8:
                continue
            g["lr"] = g["lr"]*.8
            s+=str(g["lr"])
        else:
            if g["lr"]>.00004:
            g["lr"] = g["lr"]*1.4
            s+= " - Out LR: "+str(g["lr"])
        i+=1
    print(s)
    """